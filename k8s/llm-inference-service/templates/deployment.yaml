apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "llm-inference-service.fullname" . }}
  labels:
    {{- include "llm-inference-service.labels" . | nindent 4 }}
spec:
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels:
      app: {{ include "llm-inference-service.name" . }}
  template:
    metadata:
      labels:
        app: {{ include "llm-inference-service.name" . }}
    spec:
      serviceAccountName: {{ include "llm-inference-service.serviceAccountName" . }}
      containers:
        - name: {{ include "llm-inference-service.name" . }}
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          ports:
            - containerPort: {{ .Values.service.targetPort }}
          resources:
            limits:
              cpu: {{ .Values.resources.limits.cpu }}
              memory: {{ .Values.resources.limits.memory }}
            requests:
              cpu: {{ .Values.resources.requests.cpu }}
              memory: {{ .Values.resources.requests.memory }}
          env:
            - name: BENTOML_CORS_ENABLED
              value: "true"
            - name: BENTOML_CORS_ALLOW_ORIGINS
              value: '["*"]'
            - name: BENTOML_CORS_ALLOW_METHODS
              value: '["GET", "POST", "OPTIONS"]'
            - name: BENTOML_CORS_ALLOW_HEADERS
              value: '["*"]'
            - name: BENTOML_CORS_ALLOW_CREDENTIALS
              value: "true"
          readinessProbe:
            tcpSocket:
              port: {{ .Values.service.targetPort }}
            initialDelaySeconds: 30
            periodSeconds: 10
          livenessProbe:
            tcpSocket:
              port: {{ .Values.service.targetPort }}
            initialDelaySeconds: 60
            periodSeconds: 30
